"""
Assistant Agent ‚Äì handles user info queries, bookings view, and general Q&A.

This agent has its own LLM brain and a set of information-retrieval tools.
It handles everything that isn't flight search/booking.
"""

from __future__ import annotations

import asyncio
import logging
from uuid import UUID

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.language_models import BaseChatModel

from app.agents.tools import ASSISTANT_TOOLS

logger = logging.getLogger(__name__)

# Retry config for transient LLM errors
_MAX_LLM_RETRIES = 2
_RETRY_BACKOFF_S = 2.0
_RETRYABLE_KEYWORDS = {"rate limit", "429", "quota", "resource exhausted",
                       "503", "timeout", "timed out", "deadline_exceeded",
                       "internal", "unavailable", "connection"}


def _is_retryable(exc: Exception) -> bool:
    msg = (str(exc) + str(type(exc).__name__)).lower()
    return any(kw in msg for kw in _RETRYABLE_KEYWORDS) or not str(exc).strip()


def _extract_text(content) -> str:
    """Normalize LLM response content to plain string.

    Gemini models may return content as a list of dicts
    (multi-part) instead of a simple string.
    """
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = []
        for part in content:
            if isinstance(part, str):
                parts.append(part)
            elif isinstance(part, dict):
                parts.append(part.get("text", str(part)))
            else:
                parts.append(str(part))
        return "".join(parts)
    return str(content)

ASSISTANT_AGENT_PROMPT = """B·∫°n l√† Assistant Agent ‚Äì tr·ª£ l√Ω th√¥ng tin du l·ªãch.

Nhi·ªám v·ª• c·ªßa b·∫°n:
‚Ä¢ Tra c·ª©u th√¥ng tin h√†nh kh√°ch (passengers) c·ªßa user ‚Üí G·ªçi tool: get_passengers
‚Ä¢ Xem danh s√°ch bookings v√† tr·∫°ng th√°i ‚Üí G·ªçi tool: get_bookings
‚Ä¢ ƒê·ªçc s·ªü th√≠ch chuy·∫øn bay (preferences) c·ªßa user ‚Üí G·ªçi tool: get_user_preferences
‚Ä¢ Xem l·ªãch bay (calendar events) ‚Üí G·ªçi tool: get_calendar_events
‚Ä¢ Th√™m booking v√†o Google Calendar ‚Üí G·ªçi tool: add_booking_to_calendar
‚Ä¢ G·ª≠i th√¥ng tin chuy·∫øn bay qua email ‚Üí G·ªçi tool: send_flight_info_email
‚Ä¢ Tr·∫£ l·ªùi c√°c c√¢u h·ªèi chung v·ªÅ du l·ªãch, visa, th·ªùi ti·∫øt, tips

**QUY T·∫ÆC QUAN TR·ªåNG**:
1. KHI task y√™u c·∫ßu "CALL TOOL: <tool_name>", B·∫ÆT BU·ªòC ph·∫£i g·ªçi tool ƒë√≥, KH√îNG ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng text
2. S·ª≠ d·ª•ng tools ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu khi c·∫ßn (view bookings, passengers, etc.)
3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán, d·ªÖ hi·ªÉu
4. Format th√¥ng tin booking d·∫°ng b·∫£ng/danh s√°ch r√µ r√†ng
5. V·ªõi c√¢u h·ªèi chung (kh√¥ng c·∫ßn tools), tr·∫£ l·ªùi t·ª´ ki·∫øn th·ª©c s·∫µn c√≥
6. Kh√¥ng b·ªãa ƒë·∫∑t d·ªØ li·ªáu ‚Äì n·∫øu kh√¥ng c√≥ th√¨ n√≥i r√µ
7. S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ tƒÉng tr·∫£i nghi·ªám

**X·ª¨ L√ù GOOGLE CALENDAR AUTHORIZATION**:
- Khi g·ªçi tool add_booking_to_calendar, n·∫øu tool tr·∫£ v·ªÅ `needs_authorization: true`:
  - Tool s·∫Ω cung c·∫•p `authorization_url` v√† `message`
  - B·∫ÆT BU·ªòC ph·∫£i hi·ªÉn th·ªã message v√† authorization_url cho user
  - Format response nh∆∞ sau:
    
    üîê **C·∫ßn k·∫øt n·ªëi Google Calendar**
    
    [message t·ª´ tool]
    
    üëâ **Click v√†o link b√™n d∆∞·ªõi ƒë·ªÉ k·∫øt n·ªëi:**
    [authorization_url]
    
    Sau khi k·∫øt n·ªëi xong, b·∫°n c√≥ th·ªÉ th·ª≠ l·∫°i y√™u c·∫ßu th√™m v√†o l·ªãch.

Format danh s√°ch booking:
üìã **Booking [ref]** ‚Äì Tr·∫°ng th√°i: [status]
   üí∞ [price] [currency] | üìÖ Ng√†y t·∫°o: [date]

Format th√¥ng tin h√†nh kh√°ch:
üë§ **[first_name] [last_name]**
   üõÇ Passport: [number] | üåç Qu·ªëc t·ªãch: [nationality]
"""


async def run_assistant_agent(
    llm: BaseChatModel,
    task: str,
    user_id: str,
    conversation_history: list[dict] | None = None,
    state: dict | None = None,
) -> str:
    """
    Run the Assistant Agent with the given task.

    Parameters
    ----------
    llm : BaseChatModel
        The LLM instance for this agent.
    task : str
        Task description from the Router.
    user_id : str
        Current user's UUID.
    conversation_history : list[dict] | None
        Prior conversation messages for context.
    state : dict | None
        Conversation state.

    Returns
    -------
    str ‚Äì Agent's response text.
    """
    # Build the LLM with tools bound
    llm_with_tools = llm.bind_tools(ASSISTANT_TOOLS)

    # Build messages
    messages = [SystemMessage(content=ASSISTANT_AGENT_PROMPT)]

    # Add relevant conversation history (last 6 messages)
    if conversation_history:
        for msg in conversation_history[-6:]:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))

    # Add the task with user context
    task_with_context = f"""User ID: {user_id}

Nhi·ªám v·ª•: {task}"""

    messages.append(HumanMessage(content=task_with_context))
    
    logger.info(f"Assistant Agent: task='{task}', user_id={user_id}")

    # Run agent loop (tool calling)
    max_iterations = 5
    for iteration in range(max_iterations):
        # LLM call with retry for transient errors
        response = None
        for attempt in range(_MAX_LLM_RETRIES + 1):
            try:
                response = await llm_with_tools.ainvoke(messages)
                break  # success
            except Exception as e:
                err_type = type(e).__name__
                err_msg = str(e) or "(empty error message)"
                logger.error(
                    f"Assistant Agent LLM error (iter={iteration}, attempt={attempt+1}): "
                    f"[{err_type}] {err_msg}",
                    exc_info=True,
                )
                if attempt < _MAX_LLM_RETRIES and _is_retryable(e):
                    wait = _RETRY_BACKOFF_S * (2 ** attempt)
                    logger.info(f"Retrying in {wait:.1f}s...")
                    await asyncio.sleep(wait)
                else:
                    return f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu: [{err_type}] {err_msg}"

        if response is None:
            return "‚ö†Ô∏è Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI. Vui l√≤ng th·ª≠ l·∫°i."

        # Check if the LLM wants to call tools
        if hasattr(response, "tool_calls") and response.tool_calls:
            messages.append(response)

            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]

                # Find and execute the tool
                tool_func = next(
                    (t for t in ASSISTANT_TOOLS if t.name == tool_name), None
                )
                if tool_func is None:
                    tool_result = f"Tool '{tool_name}' not found."
                else:
                    try:
                        tool_result = await tool_func.ainvoke(tool_args)
                    except Exception as e:
                        logger.error(f"Assistant tool {tool_name} error: {e}")
                        tool_result = f"Error: {str(e)}"

                # Add tool result as a ToolMessage
                from langchain_core.messages import ToolMessage
                messages.append(
                    ToolMessage(content=str(tool_result), tool_call_id=tool_call["id"])
                )
        else:
            # No more tool calls ‚Äì return the final response
            raw = response.content if hasattr(response, "content") else str(response)
            return _extract_text(raw)

    return "‚ö†Ô∏è Agent ƒë√£ x·ª≠ l√Ω qu√° nhi·ªÅu b∆∞·ªõc. Vui l√≤ng th·ª≠ l·∫°i."
